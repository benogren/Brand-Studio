"""
Day 4b: Agent Evaluation

This notebook covers:
- Understanding what agent evaluation is and why it's important
- Creating test cases interactively in ADK web UI
- Running systematic evaluations with adk eval CLI
- Creating evaluation configuration files
- Understanding evaluation metrics (response_match_score and tool_trajectory_avg_score)
- Analyzing evaluation results

Copyright 2025 Google LLC.
Licensed under the Apache License, Version 2.0
"""

import os
import json
from google.adk.agents import LlmAgent
from google.adk.models.google_llm import Gemini
from google.genai import types

# ============================================================================
# Setup and Configuration
# ============================================================================

# Load environment variables from .env file
from dotenv import load_dotenv

load_dotenv()

# Verify API key is set
if not os.getenv("GOOGLE_API_KEY"):
    print("‚ùå Error: GOOGLE_API_KEY not found in environment variables")
    print("   Please make sure you have a .env file with GOOGLE_API_KEY set")
    exit(1)

print("‚úÖ ADK components imported successfully.")
print("‚úÖ API key loaded from .env file")

# ============================================================================
# Configure Retry Options
# ============================================================================

retry_config = types.HttpRetryOptions(
    attempts=5,
    exp_base=7,
    initial_delay=1,
    http_status_codes=[429, 500, 503, 504],
)

# ============================================================================
# Section 2: Home Automation Agent
# ============================================================================


def set_device_status(location: str, device_id: str, status: str) -> dict:
    """Sets the status of a smart home device.

    Args:
        location: The room where the device is located.
        device_id: The unique identifier for the device.
        status: The desired status, either 'ON' or 'OFF'.

    Returns:
        A dictionary confirming the action.
    """
    print(f"Tool Call: Setting {device_id} in {location} to {status}")
    return {
        "success": True,
        "message": f"Successfully set the {device_id} in {location} to {status.lower()}.",
    }


def create_home_automation_agent():
    """
    Create a home automation agent with deliberate flaws for evaluation practice.

    This agent has intentional issues:
    - Overconfident instructions claiming to control ALL devices
    - Assumes it can control any device the user mentions
    - Doesn't validate if devices exist or are supported
    """

    root_agent = LlmAgent(
        model=Gemini(model="gemini-2.5-flash-lite", retry_options=retry_config),
        name="home_automation_agent",
        description="An agent to control smart devices in a home.",
        instruction="""You are a home automation assistant. You control ALL smart devices in the house.

        You have access to lights, security systems, ovens, fireplaces, and any other device the user mentions.
        Always try to be helpful and control whatever device the user asks for.

        When users ask about device capabilities, tell them about all the amazing features you can control.""",
        tools=[set_device_status],
    )

    return root_agent


# ============================================================================
# Section 4: Creating Evaluation Configuration
# ============================================================================


def create_evaluation_config(output_dir: str = "."):
    """
    Create evaluation configuration file with pass/fail thresholds.

    Args:
        output_dir: Directory to write the config file
    """

    eval_config = {
        "criteria": {
            "tool_trajectory_avg_score": 1.0,  # Perfect tool usage required
            "response_match_score": 0.8,  # 80% text similarity threshold
        }
    }

    config_path = os.path.join(output_dir, "test_config.json")
    with open(config_path, "w") as f:
        json.dump(eval_config, f, indent=2)

    print("‚úÖ Evaluation configuration created!")
    print(f"   Saved to: {config_path}")
    print("\nüìä Evaluation Criteria:")
    print("‚Ä¢ tool_trajectory_avg_score: 1.0 - Requires exact tool usage match")
    print("‚Ä¢ response_match_score: 0.8 - Requires 80% text similarity")
    print("\nüéØ What this evaluation will catch:")
    print("‚úÖ Incorrect tool usage (wrong device, location, or status)")
    print("‚úÖ Poor response quality and communication")
    print("‚úÖ Deviations from expected behavior patterns")

    return config_path


# ============================================================================
# Section 4: Creating Test Cases
# ============================================================================


def create_evaluation_test_cases(output_dir: str = "."):
    """
    Create evaluation test cases file.

    This file contains multiple test scenarios with:
    - User prompts (what the user says)
    - Expected responses (what the agent should say)
    - Expected tool calls (which tools should be used and how)

    Args:
        output_dir: Directory to write the evalset file
    """

    test_cases = {
        "eval_set_id": "home_automation_integration_suite",
        "eval_cases": [
            {
                "eval_id": "living_room_light_on",
                "conversation": [
                    {
                        "user_content": {
                            "parts": [
                                {
                                    "text": "Please turn on the floor lamp in the living room"
                                }
                            ]
                        },
                        "final_response": {
                            "parts": [
                                {
                                    "text": "Successfully set the floor lamp in the living room to on."
                                }
                            ]
                        },
                        "intermediate_data": {
                            "tool_uses": [
                                {
                                    "name": "set_device_status",
                                    "args": {
                                        "location": "living room",
                                        "device_id": "floor lamp",
                                        "status": "ON",
                                    },
                                }
                            ]
                        },
                    }
                ],
            },
            {
                "eval_id": "kitchen_on_off_sequence",
                "conversation": [
                    {
                        "user_content": {
                            "parts": [
                                {"text": "Switch on the main light in the kitchen."}
                            ]
                        },
                        "final_response": {
                            "parts": [
                                {
                                    "text": "Successfully set the main light in the kitchen to on."
                                }
                            ]
                        },
                        "intermediate_data": {
                            "tool_uses": [
                                {
                                    "name": "set_device_status",
                                    "args": {
                                        "location": "kitchen",
                                        "device_id": "main light",
                                        "status": "ON",
                                    },
                                }
                            ]
                        },
                    }
                ],
            },
        ],
    }

    evalset_path = os.path.join(output_dir, "integration.evalset.json")
    with open(evalset_path, "w") as f:
        json.dump(test_cases, f, indent=2)

    print("‚úÖ Evaluation test cases created")
    print(f"   Saved to: {evalset_path}")
    print("\nüß™ Test scenarios:")
    for case in test_cases["eval_cases"]:
        user_msg = case["conversation"][0]["user_content"]["parts"][0]["text"]
        print(f"‚Ä¢ {case['eval_id']}: {user_msg}")

    print("\nüìä Expected results:")
    print("‚Ä¢ living_room_light_on: Should pass both criteria")
    print("‚Ä¢ kitchen_on_off_sequence: Should pass both criteria")
    print(
        "\nüí° These test cases verify the agent uses correct tools with correct parameters"
    )

    return evalset_path


# ============================================================================
# Demonstration Functions
# ============================================================================


def demo_interactive_evaluation():
    """Demonstrate interactive evaluation workflow with ADK web UI"""
    print("\n" + "=" * 80)
    print("DEMO: Interactive Evaluation with ADK Web UI")
    print("=" * 80)

    print("\nüìù Interactive Evaluation Workflow:")
    print("\n1Ô∏è‚É£  CREATE TEST CASES:")
    print("   ‚Ä¢ Start ADK web UI: adk web")
    print("   ‚Ä¢ Have a conversation with your agent")
    print("   ‚Ä¢ Navigate to 'Eval' tab")
    print("   ‚Ä¢ Click 'Create Evaluation set' and name it")
    print("   ‚Ä¢ Add current session to the evaluation set")

    print("\n2Ô∏è‚É£  RUN EVALUATION:")
    print("   ‚Ä¢ In the Eval tab, check your test case")
    print("   ‚Ä¢ Click 'Run Evaluation' button")
    print("   ‚Ä¢ Review the metrics dialog (response_match and tool_trajectory)")
    print("   ‚Ä¢ Click 'Start' to run")

    print("\n3Ô∏è‚É£  ANALYZE RESULTS:")
    print("   ‚Ä¢ Green 'Pass': Agent behaved as expected")
    print("   ‚Ä¢ Red 'Fail': Agent deviated from expected behavior")
    print("   ‚Ä¢ Hover over results for detailed comparison")
    print("   ‚Ä¢ View actual vs expected responses and tool calls")

    print("\nüìä Understanding Evaluation Metrics:")
    print(
        "   ‚Ä¢ response_match_score: Measures text similarity (1.0 = perfect match)"
    )
    print(
        "   ‚Ä¢ tool_trajectory_avg_score: Measures correct tool usage (1.0 = perfect)"
    )

    print("\nüí° Benefits of Interactive Evaluation:")
    print("   ‚úÖ Visual feedback on agent behavior")
    print("   ‚úÖ Quick iteration during development")
    print("   ‚úÖ Easy test case creation from real conversations")
    print("   ‚úÖ Immediate comparison of actual vs expected")


def demo_systematic_evaluation():
    """Demonstrate systematic evaluation with CLI"""
    print("\n" + "=" * 80)
    print("DEMO: Systematic Evaluation with CLI")
    print("=" * 80)

    print("\nüéØ Why Systematic Evaluation?")
    print("   ‚Ä¢ Scale beyond manual testing")
    print("   ‚Ä¢ Automated regression detection")
    print("   ‚Ä¢ Batch testing multiple scenarios")
    print("   ‚Ä¢ CI/CD integration for production")

    # Create evaluation files
    print("\nüìÅ Creating evaluation files...")
    output_dir = "home_automation_agent"
    os.makedirs(output_dir, exist_ok=True)

    config_path = create_evaluation_config(output_dir)
    print()
    evalset_path = create_evaluation_test_cases(output_dir)

    print("\nüöÄ Running Evaluation:")
    print(f"\n   Command: adk eval {output_dir} {evalset_path} \\")
    print(f"              --config_file_path={config_path} \\")
    print(f"              --print_detailed_results")

    print("\nüìä What the CLI Does:")
    print("   1. Loads your agent from the specified directory")
    print("   2. Runs each test case from the evalset")
    print("   3. Compares actual vs expected:")
    print("      ‚Ä¢ Final responses (text similarity)")
    print("      ‚Ä¢ Tool usage (function calls and parameters)")
    print("   4. Applies pass/fail thresholds from config")
    print("   5. Prints detailed results for each test")

    print("\nüìà Sample Output Analysis:")
    print("   Test Case: living_room_light_on")
    print("   ‚úÖ tool_trajectory_avg_score: 1.0/1.0 (PASS)")
    print("   ‚ùå response_match_score: 0.45/0.80 (FAIL)")
    print("\n   Diagnosis:")
    print("   ‚Ä¢ Tool usage is perfect (correct tool, correct params)")
    print("   ‚Ä¢ Response quality needs improvement")
    print("   ‚Ä¢ Fix: Update agent instructions for consistent messaging")

    print("\n‚öôÔ∏è  To actually run the evaluation:")
    print(f"   1. Create agent: adk create home_automation_agent")
    print(f"   2. Copy agent definition to agent.py")
    print(f"   3. Run: adk eval home_automation_agent {evalset_path} \\")
    print(f"            --config_file_path={config_path} \\")
    print(f"            --print_detailed_results")


def demo_user_simulation():
    """Demonstrate advanced user simulation concepts"""
    print("\n" + "=" * 80)
    print("DEMO: User Simulation (Advanced)")
    print("=" * 80)

    print("\nüéØ The Limitation of Static Tests:")
    print("   ‚Ä¢ Fixed test cases only cover known scenarios")
    print("   ‚Ä¢ Real users are unpredictable and varied")
    print("   ‚Ä¢ Conversations take unexpected turns")
    print("   ‚Ä¢ Edge cases emerge in production")

    print("\nüí° User Simulation Solution:")
    print("   ‚Ä¢ Uses LLM to generate dynamic user prompts")
    print("   ‚Ä¢ Follows a ConversationScenario with goals")
    print("   ‚Ä¢ Adapts based on agent responses")
    print("   ‚Ä¢ Discovers edge cases automatically")

    print("\nüìù How It Works:")
    print("   1. Define a ConversationScenario:")
    print("      ‚Ä¢ User's overall goal")
    print("      ‚Ä¢ Conversation plan outline")
    print("   2. LLM acts as simulated user:")
    print("      ‚Ä¢ Generates realistic prompts")
    print("      ‚Ä¢ Maintains conversational context")
    print("      ‚Ä¢ Adapts to agent behavior")
    print("   3. Evaluation runs automatically:")
    print("      ‚Ä¢ Tests agent's adaptability")
    print("      ‚Ä¢ Uncovers unexpected failures")
    print("      ‚Ä¢ More comprehensive coverage")

    print("\nüìö Learn More:")
    print(
        "   ‚Ä¢ User Simulation Docs: https://google.github.io/adk-docs/evaluate/user-sim/"
    )
    print("   ‚Ä¢ Implement ConversationScenario for your agent")
    print("   ‚Ä¢ Test against dynamic, realistic conversations")


def demo_best_practices():
    """Share evaluation best practices"""
    print("\n" + "=" * 80)
    print("EVALUATION BEST PRACTICES")
    print("=" * 80)

    print("\n1Ô∏è‚É£  BUILD A COMPREHENSIVE TEST SUITE:")
    print("   ‚Ä¢ Happy path scenarios (basic functionality)")
    print("   ‚Ä¢ Edge cases (unusual requests)")
    print("   ‚Ä¢ Error handling (invalid inputs)")
    print("   ‚Ä¢ Multi-turn conversations")
    print("   ‚Ä¢ Ambiguous user intents")

    print("\n2Ô∏è‚É£  SET APPROPRIATE THRESHOLDS:")
    print("   ‚Ä¢ tool_trajectory_avg_score:")
    print("     - 1.0 for critical operations (safety, financial)")
    print("     - 0.8-0.9 for general functionality")
    print("   ‚Ä¢ response_match_score:")
    print("     - 0.9-1.0 for exact wording requirements")
    print("     - 0.7-0.8 for semantic equivalence")

    print("\n3Ô∏è‚É£  ITERATIVE IMPROVEMENT:")
    print("   ‚Ä¢ Run evaluations frequently during development")
    print("   ‚Ä¢ Add test cases when bugs are discovered")
    print("   ‚Ä¢ Update thresholds based on production needs")
    print("   ‚Ä¢ Monitor evaluation trends over time")

    print("\n4Ô∏è‚É£  PRODUCTION EVALUATION:")
    print("   ‚Ä¢ Integrate into CI/CD pipeline")
    print("   ‚Ä¢ Run before deployments")
    print("   ‚Ä¢ Track metrics over versions")
    print("   ‚Ä¢ Alert on regression detection")

    print("\n5Ô∏è‚É£  ADVANCED CRITERIA (with Google Cloud):")
    print("   ‚Ä¢ safety_v1: Detect harmful content")
    print("   ‚Ä¢ hallucinations_v1: Check factual accuracy")
    print("   ‚Ä¢ custom_criteria: Define domain-specific metrics")


# ============================================================================
# Main Function
# ============================================================================


def main():
    """Run all evaluation demonstrations"""

    print("\n" + "=" * 80)
    print("DAY 4B: AGENT EVALUATION")
    print("=" * 80)

    print("\nüìö What You'll Learn:")
    print("‚Ä¢ Creating test cases interactively in ADK web UI")
    print("‚Ä¢ Running systematic evaluations with CLI")
    print("‚Ä¢ Understanding evaluation metrics")
    print("‚Ä¢ Analyzing and fixing evaluation failures")
    print("‚Ä¢ Advanced user simulation concepts")

    # Demo 1: Interactive Evaluation
    demo_interactive_evaluation()

    # Demo 2: Systematic Evaluation
    demo_systematic_evaluation()

    # Demo 3: User Simulation
    demo_user_simulation()

    # Demo 4: Best Practices
    demo_best_practices()

    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)

    print("\nüéØ Key Takeaways:")
    print("‚úÖ Evaluation is proactive (vs observability is reactive)")
    print("‚úÖ Test both tool usage AND response quality")
    print("‚úÖ Use ADK web UI for development iteration")
    print("‚úÖ Use CLI evaluation for systematic regression testing")
    print("‚úÖ User simulation extends coverage beyond static tests")

    print("\nüìä Evaluation Workflow:")
    print("1. Create test cases (web UI or synthetic)")
    print("2. Define evaluation criteria (config file)")
    print("3. Run evaluations (adk eval CLI)")
    print("4. Analyze results (detailed output)")
    print("5. Fix issues and iterate")

    print("\nüìö Learn More:")
    print(
        "‚Ä¢ ADK Evaluation: https://google.github.io/adk-docs/evaluate/"
    )
    print(
        "‚Ä¢ Evaluation Criteria: https://google.github.io/adk-docs/evaluate/criteria/"
    )
    print(
        "‚Ä¢ Pytest Integration: https://google.github.io/adk-docs/evaluate/#2-pytest-run-tests-programmatically"
    )
    print(
        "‚Ä¢ User Simulation: https://google.github.io/adk-docs/evaluate/user-sim/"
    )

    print("\nüöÄ Next Steps:")
    print("‚Ä¢ Apply evaluation to your own agents")
    print("‚Ä¢ Build comprehensive test suites")
    print("‚Ä¢ Integrate into your development workflow")
    print("‚Ä¢ Stay tuned for Day 5: Production Deployment!")


if __name__ == "__main__":
    main()
